{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "PAN-21-es.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "pip install emoji"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Du8Cn2ledVF2",
        "outputId": "3af047cc-2e1c-4e08-bebc-a967c79284af"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import os\r\n",
        "import sys\r\n",
        "import re\r\n",
        "import xml.etree.ElementTree as ET\r\n",
        "import emoji\r\n",
        "from html import unescape\r\n",
        "from emoji import UNICODE_EMOJI\r\n",
        "\r\n",
        "def listXML(path):\r\n",
        "    #return a list with the xml name files\r\n",
        "    # r=root, d=directories, f = files\r\n",
        "    xml_list = []\r\n",
        "    for r, d, f in os.walk(path):\r\n",
        "        for file in f:\r\n",
        "            if '.xml' in file:\r\n",
        "                xml_list.append(file)\r\n",
        "    return xml_list\r\n",
        "\r\n",
        "\r\n",
        "def concatenateTweets(xml_list,source_path):\r\n",
        "    id_authors = [] # ids of authors\r\n",
        "    x_train = [] #each element of the list is composed of all the tweets concatenated which have been written by an author \r\n",
        "    for p in xml_list:\r\n",
        "        h, tail = os.path.split(p) #return (\"\",'id.xml')\r\n",
        "        tail = tail.split(\".\")\r\n",
        "        author = tail[0]\r\n",
        "        id_authors.append(author)\r\n",
        "        tree = ET.parse(source_path+str(p))\r\n",
        "        root = tree.getroot()\r\n",
        "        for child in root:\r\n",
        "            xi = []\r\n",
        "            for ch in child:\r\n",
        "                xi.append(ch.text)\r\n",
        "            content = ' '.join(xi)\r\n",
        "            x_train.append(content)\r\n",
        "    return id_authors, x_train\r\n",
        "\r\n",
        "\r\n",
        "def hateLabels(path,spa_id_authors):\r\n",
        "    text = open(path,\"r\")\r\n",
        "    data = text.read().split(\"\\n\")\r\n",
        "    labels = {}\r\n",
        "    test = [] #class labels for authors\r\n",
        "    for line in data:\r\n",
        "        l = line.split(\":::\")\r\n",
        "        if len(l) > 1:\r\n",
        "            labels[l[0]] = l[1]\r\n",
        "    for author in spa_id_authors:\r\n",
        "        test.append(labels[author])\r\n",
        "    return test\r\n",
        "\r\n",
        "def emoji2word(text):\r\n",
        "    return emoji.demojize(text)\r\n",
        "\r\n",
        "def cleaning_v1(tweet_lista):#english tweets\r\n",
        "    cleaned_feed_v1=[]\r\n",
        "    for feed in tweet_lista:\r\n",
        "        feed = feed.lower()\r\n",
        "        feed = re.sub('[^0-9a-z #@]', \"\", feed)\r\n",
        "        feed = re.sub('[\\n]', \" \", feed)\r\n",
        "        cleaned_feed_v1.append(feed)\r\n",
        "    return cleaned_feed_v1\r\n",
        "\r\n",
        "def cleanTweets(tweet_list):#spanish tweets\r\n",
        "    clean_list = []\r\n",
        "    for t in tweet_list:\r\n",
        "        t = t.lower()\r\n",
        "        t = re.sub(r'\\d+', \"xnumber\", t)\r\n",
        "        t = re.sub('#user#', '', t)\r\n",
        "        t = re.sub('#url#','xurl',t)\r\n",
        "        t = re.sub('#rt#', '', t)\r\n",
        "        t = re.sub('#hashtag#','',t)\r\n",
        "        t = emoji2word(t) \r\n",
        "        t = re.sub('[,.\\'\\\"\\‘\\’\\”\\“]', '', t)\r\n",
        "        t = re.sub(r'(?<![?!:;/])([:\\'\\\";.,?()/!])(?= )','',t) \r\n",
        "        t = re.sub(r'([a-z\\'-\\’]+)', r'\\1 ', t)\r\n",
        "        t = re.sub('[\\n]', ' ', t)\r\n",
        "        t = ' '.join(t.split())\r\n",
        "        clean_list.append(t)\r\n",
        "    return clean_list"
      ],
      "outputs": [],
      "metadata": {
        "id": "QivN3CwfdZlI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load Data**"
      ],
      "metadata": {
        "id": "dviYXo3frNZI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "source_path = '/content/drive/MyDrive/PAN-21/dataset'\r\n",
        "#spanish data\r\n",
        "spa_xml_list = listXML(source_path+'/es/')\r\n",
        "spa_id_authors, spa_x_train = concatenateTweets(spa_xml_list,source_path+'/es/') \r\n",
        "spa_y_train = hateLabels(source_path+'/es/truth.txt',spa_id_authors)# we obtain labels of each autor\r\n",
        "spa_clean_x_train = cleanTweets(spa_x_train) \r\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "LJwe_ezlddot"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We shuffle the dataset."
      ],
      "metadata": {
        "id": "vxqTf4wWbl7u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import random\n",
        "x_train = spa_clean_x_train.copy()\n",
        "y_train = spa_y_train.copy()\n",
        "mix = list(zip(x_train,y_train))\n",
        "random.shuffle(mix)\n",
        "x_train, y_train = zip(*mix)\n",
        "x_train = list(x_train)\n",
        "y_train = list(y_train)"
      ],
      "outputs": [],
      "metadata": {
        "id": "hAPE_yZzav32"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SVM MODEL WITH TF-IDF VECTORIZATION.**\n",
        "First, we use GridSearch with the objective to find the best hyperparameter configuration to SVM and Tf-idf vectorization of each set of phrases.(This process takes long time depending on the number of parameters used to test)\n"
      ],
      "metadata": {
        "id": "-G3noiC0eLsn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "import math\n",
        "param_grid = {\n",
        "    'vect__ngram_range':[(1,1),(1,2),(1,3),(1,5),(1,6)],\n",
        "    'vect__min_df':[0.1,0.3,0.6,0.8,2,4,6,8,12],\n",
        "    'vect__analyzer':['word','char_wb'],\n",
        "    'svm__C':[100,10,1],\n",
        "    'svm__kernel':['linear','rbf','poly']\n",
        "}\n",
        "tr_x = x_train.copy()\n",
        "tr_y = y_train.copy()\n",
        "pipe = Pipeline([('vect', TfidfVectorizer(sublinear_tf=True, use_idf=True, smooth_idf=True)),\n",
        "                ('svm', SVC())])\n",
        "search = GridSearchCV(pipe, param_grid, n_jobs=-1)\n",
        "search.fit(tr_x,tr_y)\n",
        "print(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\n",
        "print(search.best_params_)"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CeC6EYw0e9tx",
        "outputId": "40a0985b-20f8-4642-a3c8-544253a7b9c2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once we have obtained the best configuration, we test this configuration through Cross Validation and furthermore, we finish to adjust the model by making some small variations in the hyperparameters."
      ],
      "metadata": {
        "id": "3VVi73pSf134"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "import math\n",
        "corpus_particionado_x_train=[]\n",
        "corpus_particionado_y_train=[]\n",
        "cv = 10\n",
        "num= int(len(x_train)/cv)\n",
        "for i in range(cv):\n",
        "    corpus_particionado_x_train.append(x_train[i*num:i*num + num])\n",
        "    corpus_particionado_y_train.append(y_train[i*num:i*num + num])\n",
        "print(len(corpus_particionado_x_train))\n",
        "print(len(corpus_particionado_y_train))"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLrlke2jfyO9",
        "outputId": "e7ff38d1-50fe-4a92-c5d8-35d6fde66cc1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "import math\n",
        "resultados_svm = []\n",
        "for i in range(cv):\n",
        "    tr_x = corpus_particionado_x_train.copy()\n",
        "    te_x = tr_x[i]\n",
        "    tr_y = corpus_particionado_y_train.copy()\n",
        "    te_y = tr_y[i]\n",
        "    del tr_x[i]\n",
        "    del tr_y[i]\n",
        "    tr_x = [valor for sublista in tr_x for valor in sublista]\n",
        "    tr_y = [valor for sublista in tr_y for valor in sublista]\n",
        "    clasificadorSVC = Pipeline([('vect', TfidfVectorizer(ngram_range=(1,6), min_df=8, sublinear_tf=True, use_idf=True, smooth_idf=True)),\n",
        "                ('svm', SVC(C=100, kernel='linear', probability=True))])\n",
        "    clasificadorSVC.fit(tr_x,tr_y)\n",
        "    print(clasificadorSVC.score(te_x,te_y))\n",
        "    resultados_svm.append(clasificadorSVC.score(te_x,te_y))\n",
        "resultados_svm\n",
        "precision_media_svm = sum(resultados_svm)/len(resultados_svm)\n",
        "aux = 0\n",
        "print(\"Average accuracy with SVM is %.4f  \" %(precision_media_svm))"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2s6_yQof8cd",
        "outputId": "39abd54d-3ea8-44ac-f73a-b681ad037b73"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In addition, if we want to use TF-IDF of char and word n-grams as features, we have to use FeatureUnion to combine both in one output and use it with the SVM.\n"
      ],
      "metadata": {
        "id": "2XlMYNt9lkEm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "resultados_svm_mix = []\n",
        "for i in range(cv):\n",
        "    tr_x = corpus_particionado_x_train.copy()\n",
        "    te_x = tr_x[i]\n",
        "    tr_y = corpus_particionado_y_train.copy()\n",
        "    te_y = tr_y[i]\n",
        "    del tr_x[i]\n",
        "    del tr_y[i]\n",
        "    tr_x = [valor for sublista in tr_x for valor in sublista]\n",
        "    tr_y = [valor for sublista in tr_y for valor in sublista]\n",
        "    estimators =[('word',TfidfVectorizer(ngram_range=(1,6), min_df=8, sublinear_tf=True, use_idf=True, smooth_idf=True)),('char',TfidfVectorizer(ngram_range=(1,6), min_df=0.3, sublinear_tf=True, use_idf=True,analyzer = 'char_wb', smooth_idf=True))]\n",
        "    combined = FeatureUnion(estimators)\n",
        "    clasificadorSVCmix = Pipeline([(('vect', combined)),\n",
        "                ('svm', SVC(C=100, kernel='linear', probability=True))])\n",
        "    clasificadorSVCmix.fit(tr_x,tr_y)\n",
        "    print(clasificadorSVCmix.score(te_x,te_y))\n",
        "    resultados_svm_mix.append(clasificadorSVCmix.score(te_x,te_y))\n",
        "resultados_svm_mix\n",
        "precision_media_svm_mix = sum(resultados_svm_mix)/len(resultados_svm_mix)\n",
        "print(\"Average accuracy with SVM using tfidf of words y chars combined is %.4f \" %(precision_media_svm_mix))"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_SZ8G-V7lakz",
        "outputId": "bc72b135-0566-481c-eef8-8d903ff309fe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **RANDOM FOREST**"
      ],
      "metadata": {
        "id": "tbeCPJARm0sL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "param_grid = {\n",
        "    'vect__ngram_range':[(1,1),(1,2),(1,3)],\n",
        "    'vect__min_df':[0.1,0.3,0.6,0.8,2,6,8],\n",
        "    'vect__analyzer':['word'],\n",
        "    'rf__n_estimators':[10,100,150,200],\n",
        "    'rf__criterion':['gini'],\n",
        "    'rf__min_samples_leaf':[1,2,4,6]\n",
        "}\n",
        "tr_rf_x = x_train.copy()\n",
        "tr_rf_y = y_train.copy()\n",
        "pipe = Pipeline([('vect', TfidfVectorizer(sublinear_tf=True, use_idf=True, smooth_idf=True)),\n",
        "                 ('rf',RandomForestClassifier())])\n",
        "search = GridSearchCV(pipe, param_grid, n_jobs=-1)\n",
        "search.fit(tr_rf_x,tr_rf_y)\n",
        "print(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\n",
        "print(search.best_params_)"
      ],
      "outputs": [],
      "metadata": {
        "id": "BjZ4V68Nn-EH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbad7f69-2019-4a75-8960-1e82e5a34255"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "resultados_rf = []\n",
        "for i in range(cv):\n",
        "    tr_x = corpus_particionado_x_train.copy()\n",
        "    te_x = tr_x[i]\n",
        "    tr_y = corpus_particionado_y_train.copy()\n",
        "    te_y = tr_y[i]\n",
        "    del tr_x[i]\n",
        "    del tr_y[i]\n",
        "    tr_x = [valor for sublista in tr_x for valor in sublista]\n",
        "    tr_y = [valor for sublista in tr_y for valor in sublista]\n",
        "    clasificadorRF = Pipeline([('vect', TfidfVectorizer(ngram_range=(1,2), min_df=8, sublinear_tf=True, use_idf=True, smooth_idf=True)),\n",
        "                ('RF',RandomForestClassifier(n_estimators=100, min_samples_leaf=1, criterion='gini'))])\n",
        "    clasificadorRF.fit(tr_x,tr_y)\n",
        "    print(clasificadorRF.score(te_x,te_y))\n",
        "    resultados_rf.append(clasificadorRF.score(te_x,te_y))\n",
        "resultados_rf\n",
        "precision_media_rf = sum(resultados_rf)/len(resultados_rf)\n",
        "aux = 0\n",
        "for res in resultados_rf:\n",
        "    aux  += (res-precision_media_rf)**2 \n",
        "varianza = aux/cv\n",
        "ic_medio_rf = 1.96 * math.sqrt(varianza)\n",
        "print(\"Average accuracy with rf is %.4f and its ic %.4f \" %(precision_media_rf,ic_medio_rf))"
      ],
      "outputs": [],
      "metadata": {
        "id": "00KPZglgm4HB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14e07472-ca10-43c8-9539-77bf1f02bdbd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **XGBoost**"
      ],
      "metadata": {
        "id": "wkHDAavX6mMj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "import math\n",
        "param_grid = {\n",
        "    'vect__ngram_range':[(1,2)],\n",
        "    'vect__min_df':[8],\n",
        "    'vect__analyzer':['word'],\n",
        "    \"xgboost__eta\":[.01,.1,.3,1],\n",
        "    \"xgboost__n_estimators\":[100,200,300],\n",
        "    \"xgboost__max_depth\": [4,5,6],\n",
        "    \"xgboost__subsample\": [.6,.8],\n",
        "    \"xgboost__colsample_bytree\":[.5,.7]\n",
        "}\n",
        "tr_x = x_train.copy()\n",
        "tr_y = y_train.copy()\n",
        "pipe = Pipeline([('vect', TfidfVectorizer(sublinear_tf=True, use_idf=True, smooth_idf=True)),\n",
        "                ('xgboost', xgb.XGBClassifier(random_state=0))])\n",
        "search = GridSearchCV(pipe, param_grid, n_jobs=-1)\n",
        "search.fit(tr_x,tr_y)\n",
        "print(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\n",
        "print(search.best_params_)"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCLxrgTq6rWO",
        "outputId": "8a108562-e5f9-422a-e735-40e149552572"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import xgboost as xgb\n",
        "resultados_xgb = []\n",
        "for i in range(cv):\n",
        "    tr_x = corpus_particionado_x_train.copy()\n",
        "    te_x = tr_x[i]\n",
        "    tr_y = corpus_particionado_y_train.copy()\n",
        "    te_y = tr_y[i]\n",
        "    del tr_x[i]\n",
        "    del tr_y[i]\n",
        "    tr_x = [valor for sublista in tr_x for valor in sublista]\n",
        "    tr_y = [valor for sublista in tr_y for valor in sublista]\n",
        "    clasificadorxgb = Pipeline([('vect', TfidfVectorizer(ngram_range=(1,2), min_df=8, sublinear_tf=True, use_idf=True, smooth_idf=True)),\n",
        "                ('xgboost', xgb.XGBClassifier(colsample_bytree= 0.7, eta= 0.01,max_depth=6,n_estimators=200,subsample=0.7))])\n",
        "    clasificadorxgb.fit(tr_x,tr_y)\n",
        "    print(clasificadorxgb.score(te_x,te_y))\n",
        "    resultados_xgb.append(clasificadorxgb.score(te_x,te_y))\n",
        "resultados_xgb\n",
        "precision_media_xgb = sum(resultados_xgb)/len(resultados_xgb)\n",
        "aux = 0\n",
        "for res in resultados_xgb:\n",
        "    aux  += (res-precision_media_xgb)**2 \n",
        "varianza = aux/cv\n",
        "ic_medio_xgb = 1.96 * math.sqrt(varianza)\n",
        "print(\"Average accuracy with xboost is %.4f and its ic %.4f \" %(precision_media_xgb,ic_medio_xgb))"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jClzBCnZ7q8j",
        "outputId": "33315a12-0742-44df-c98c-460b36284925"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Ensembled models**"
      ],
      "metadata": {
        "id": "0EJ4OJllACMn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "resultados_ens = []\n",
        "for i in range(cv):\n",
        "    tr_x = corpus_particionado_x_train.copy()\n",
        "    te_x = tr_x[i]\n",
        "    tr_y = corpus_particionado_y_train.copy()\n",
        "    te_y = tr_y[i]\n",
        "    del tr_x[i]\n",
        "    del tr_y[i]\n",
        "    tr_x = [valor for sublista in tr_x for valor in sublista]\n",
        "    tr_y = [valor for sublista in tr_y for valor in sublista]\n",
        "    clasificadorRF = Pipeline([('vect', TfidfVectorizer(ngram_range=(1,2), min_df=8, sublinear_tf=True, use_idf=True, smooth_idf=True)),\n",
        "                ('RF',RandomForestClassifier(n_estimators=100, min_samples_leaf=1, criterion='gini'))])\n",
        "    clasificadorSVC = Pipeline([('vect', TfidfVectorizer(ngram_range=(1,2), min_df=8, sublinear_tf=True, use_idf=True, smooth_idf=True)),\n",
        "                ('svm', SVC(C=100, kernel='linear', probability=True))])\n",
        "    ensembled = VotingClassifier(estimators=[('svm', clasificadorSVC), ('RF', clasificadorRF),],voting='soft', weights=[4,1])\n",
        "    clasificadorSVC.fit(tr_x,tr_y)\n",
        "    clasificadorRF.fit(tr_x,tr_y)\n",
        "    ensembled.fit(tr_x,tr_y)\n",
        "    resultados_ens.append(ensembled.score(te_x,te_y))\n",
        "resultados_ens\n",
        "precision_media_ens = sum(resultados_ens)/len(resultados_ens)\n",
        "aux = 0\n",
        "for res in resultados_ens:\n",
        "    aux  += (res-precision_media_ens)**2 \n",
        "varianza = aux/cv\n",
        "ic_medio_ens = 1.96 * math.sqrt(varianza)\n",
        "print(\"Average accuracy with ensembled model is %.4f and its ic %.4f \" %(precision_media_ens,ic_medio_ens))"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4eLw4zNAIr5",
        "outputId": "251eaf67-3831-4e9f-9381-b60f7b4b746d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Pre-trained BERT**"
      ],
      "metadata": {
        "id": "EhcniMm2wu32"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!pip install -q tensorflow-text\n",
        "!pip install -q tf-models-official\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "from official.nlp import optimization  # to create AdamW optmizer\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWOK0B9TwqkC",
        "outputId": "2698c4a1-5169-4a95-ae40-3bca9758477b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case, we have created the classifier model using multi_cased BERT because the text is written in spanish. \n",
        "</br>\n",
        "Due to this, we have implemented the preprocessing_layer  with \"https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3\" and the encoder with https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3 .\n",
        "</br>\n",
        "We can find other pre-trained BERT models on https://tfhub.dev/google/collections/bert/1 ."
      ],
      "metadata": {
        "id": "Uu4hbiany3eE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def build_classifier_model():\n",
        "  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
        "  preprocessing_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3\", name='preprocessing')\n",
        "  encoder_inputs = preprocessing_layer(text_input)\n",
        "  encoder = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3\", trainable=True, name='BERT_encoder')\n",
        "  outputs = encoder(encoder_inputs)\n",
        "  net = outputs['pooled_output']\n",
        "  net = tf.keras.layers.Dropout(0.1)(net)\n",
        "  net = tf.keras.layers.Dense(64, activation='relu', name='dense')(net)\n",
        "  net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n",
        "  return tf.keras.Model(text_input, net)"
      ],
      "outputs": [],
      "metadata": {
        "id": "D9Zp-Z4-xnCz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "classifier_model = build_classifier_model()\n",
        "tf.keras.utils.plot_model(classifier_model)"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "OJ4dxUvDxsO8",
        "outputId": "3c1b364b-bc2b-46bc-df5a-7022e79e674d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next step is to convert the data to a correct format. We also split the data into 90% train and 10% test in order to prove the accuracy provided by the created model."
      ],
      "metadata": {
        "id": "0n3nxN4d0uXZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train = spa_clean_x_train.copy()\n",
        "y_train = spa_y_train.copy()\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(x_train,y_train, test_size = 0.10, random_state = 42)\n",
        "for i in range(len(Y_train)):\n",
        "    Y_train[i] = int(Y_train[i])\n",
        "for i in range(len(Y_test)):\n",
        "    Y_test[i] = int(Y_test[i])\n",
        "batch_size = 32\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((X_train, Y_train)).shuffle(10000).batch(batch_size)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((X_test, Y_test)).shuffle(10000).batch(batch_size)"
      ],
      "outputs": [],
      "metadata": {
        "id": "zAKCyjlqyMKI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "metrics = tf.metrics.BinaryAccuracy()"
      ],
      "outputs": [],
      "metadata": {
        "id": "PtVE3cjXybxe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "epochs = 15\n",
        "steps_per_epoch = tf.data.experimental.cardinality(train_ds).numpy()\n",
        "num_train_steps = steps_per_epoch * epochs\n",
        "num_warmup_steps = int(0.1*num_train_steps)\n",
        "\n",
        "init_lr = 3e-5\n",
        "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "                                          num_train_steps=num_train_steps,\n",
        "                                          num_warmup_steps=num_warmup_steps,\n",
        "                                          optimizer_type='adamw')"
      ],
      "outputs": [],
      "metadata": {
        "id": "BP_9gyVDye4L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "classifier_model.compile(optimizer=optimizer,\n",
        "                         loss=loss,\n",
        "                         metrics=metrics)"
      ],
      "outputs": [],
      "metadata": {
        "id": "8HmBEjxMykWk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "history = classifier_model.fit(x=train_ds,\n",
        "                               epochs=epochs)"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKx9Ug0myp9q",
        "outputId": "5cbc2a4b-a6fc-4638-bc00-747f5d52a09c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "loss, accuracy = classifier_model.evaluate(test_ds)\n",
        "print(f'Loss: {loss}')\n",
        "print(f'Accuracy: {accuracy}')"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CcuAF7f9yyEw",
        "outputId": "559e1b9b-8f3e-4241-f3c2-39a3af319d79"
      }
    }
  ]
}